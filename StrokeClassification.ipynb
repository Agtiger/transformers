{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agtiger/transformers/blob/main/StrokeClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT_bs7ieOnbm"
      },
      "outputs": [],
      "source": [
        "stable = True # Set to True for latest pip version or False for main branch in GitHub\n",
        "!pip install tsai -t /content/drive/MyDrive/TSAI_NEW\n",
        "!pip install h5py -t /content/drive/MyDrive/TSAI_NEW\n",
        "!pip install -U aeon -t /content/drive/MyDrive/AEON\n",
        "import sys\n",
        "print('\\n'.join(sys.path))\n",
        "sys.path.append('/content/drive/MyDrive/AEON')\n",
        "from aeon.datasets import load_arrow_head, load_basic_motions\n",
        "from aeon.transformations.collection.feature_based import TSFreshFeatureExtractor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stable = True # Set to True for latest pip version or False for main branch in GitHub\n",
        "!pip install tsai -t /TSAI_NEW\n",
        "!pip install h5py -t /TSAI_NEW"
      ],
      "metadata": {
        "id": "pA3lfDceKeuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu_pVBB5w0Yr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print('\\n'.join(sys.path))\n",
        "sys.path.append('/content/drive/MyDrive/TSAI_NEW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8axzr8eKwBeQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive if the file is stored there\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/TSAI_NEW/data/Data_Marker.mat'  # Update this path\n",
        "file_path2 = '/content/drive/MyDrive/TSAI_NEW/data/target.mat'  # Update this path\n",
        "# Load the .mat file\n",
        "try:\n",
        "    mat_data = scipy.io.loadmat(file_path)\n",
        "    mat_data2 = scipy.io.loadmat(file_path2)\n",
        "    print(\"Keys in the .mat file:\", mat_data.keys())  # Show available keys\n",
        "    print(\"Keys in the .mat file:\", mat_data2.keys())  # Show available keys\n",
        "    # Access the variable (update 'variable1' to your actual variable name)\n",
        "    variable_name = 'Data_Marker'  # Change this to your actual variable name\n",
        "    variable_name2 = 'target'  # Change this to your actual variable name\n",
        "    if variable_name in mat_data and variable_name2 in mat_data2:\n",
        "        data = mat_data[variable_name]\n",
        "        data2 = mat_data2[variable_name2]\n",
        "        print(\"Data shape:\", data.shape)\n",
        "        print(\"Data2 shape:\", data2.shape)\n",
        "    else:\n",
        "        print(f\"Variable '{variable_name}' not found in the file.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t-Sx6P1s8AKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdhN4B6N04X3"
      },
      "outputs": [],
      "source": [
        "from tsai.all import *\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import *\n",
        "import sklearn.metrics as skm\n",
        "from fastai.metrics import *\n",
        "from fastai.callback.all import EarlyStoppingCallback\n",
        "from fastai.vision.all import *\n",
        "from fastai.callback.tracker import EarlyStoppingCallback\n",
        "my_setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDxJO8hq3PXb"
      },
      "outputs": [],
      "source": [
        "X=data\n",
        "y=np.squeeze(data2)\n",
        "#splits = get_splits(y, valid_size=.2, test_size=0, random_state=23, stratify=True, shuffle=True)\n",
        "\n",
        "splits = get_splits(y, n_splits=5, shuffle=True, stratify=True)\n",
        "for s in splits:\n",
        "    print(np.mean(y[s[0]]), np.mean(y[s[1]]))\n",
        "splits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MySuperCustomModel4(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=32, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.5, bidirectional=True, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=32, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        super(MySuperCustomModel4,self).__init__()\n",
        "        #self.lstm = nn.LSTM(input_size=c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.rnn = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        nf = ifnone(nf, nb_filters)\n",
        "        self.block = XceptionBlock(hidden_size*2, nf, **kwargs)\n",
        "        self.head_nf = nf * 32\n",
        "        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(adaptive_size),\n",
        "                                  ConvBlock(self.head_nf, self.head_nf//2, 1),\n",
        "                                  ConvBlock(self.head_nf//2, self.head_nf//4, 1),\n",
        "                                  ConvBlock(self.head_nf//4, c_out, 1),\n",
        "                                  GAP1d(1))\n",
        "    def forward(self, x):\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        output, _ = self.rnn(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "        output2=self.shuffle(output)\n",
        "        x = self.block(output2)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WTzc2TUGNBom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MySuperCustomModel5(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=True, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        super(MySuperCustomModel5,self).__init__()\n",
        "        #self.lstm = nn.LSTM(input_size=c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.LSTM = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        #self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        #Xception\n",
        "        nf = ifnone(nf, nb_filters)\n",
        "        self.block = XceptionBlock(c_in, nf, **kwargs)\n",
        "        self.head_nf = nf * 32\n",
        "        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(adaptive_size),\n",
        "                                  ConvBlock(self.head_nf, self.head_nf//2, 1),\n",
        "                                  ConvBlock(self.head_nf//2, self.head_nf//4, 1),\n",
        "                                  ConvBlock(self.head_nf//4, c_out, 1),\n",
        "                                  GAP1d(1))\n",
        "        # Common\n",
        "        self.concat = Concat()\n",
        "        self.fc_dropout = nn.Dropout(fc_dropout) if fc_dropout else noop\n",
        "        self.fc = nn.Linear(hidden_size * (1 + bidirectional) + c_out, c_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # xCEPTION\n",
        "        x1 = self.block(x)\n",
        "        y = self.head(x1)\n",
        "        #print(x.shape)\n",
        "\n",
        "        # LSTM\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        #print(rnn_input.shape)\n",
        "        output, _ = self.LSTM(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "        #print(last_out.shape)\n",
        "\n",
        "        # Concat\n",
        "        x_con = self.concat([last_out, y])\n",
        "        x_con = self.fc_dropout(x_con)\n",
        "        x_con = self.fc(x_con)\n",
        "\n",
        "        return x_con"
      ],
      "metadata": {
        "id": "sAppTP0eSq7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = get_splits(y, n_splits=5, shuffle=True, stratify=True)\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(XceptionTime, {})]\n",
        "arch_name='LSTTM_Xception'\n",
        "results = pd.DataFrame(columns=['kk', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  model = MySuperCustomModel4\n",
        "  learn = ts_learner(dls, model, metrics=accuracy)\n",
        "  start = time.time()\n",
        "  cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=12), ShowGraph()]\n",
        "  #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "  learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "  kk=kk+1\n",
        "  learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "  elapsed = time.time() - start\n",
        "  vals = learn.recorder.values[-1]\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  results.loc[kk] = [kk, vals[0], vals[1], ac,pre,f1,auc,recal, int(elapsed)]\n",
        "  results.sort_values(by='kk', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "  clear_output()\n",
        "  display(results)"
      ],
      "metadata": {
        "id": "fBDtz17bXFP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = get_splits(y, n_splits=5, shuffle=True, stratify=True)\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(XceptionTime, {})]\n",
        "arch_name='LSTTM_Xception_contact'\n",
        "results = pd.DataFrame(columns=['kk', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  model = MySuperCustomModel5\n",
        "  learn = ts_learner(dls, model, metrics=accuracy)\n",
        "  start = time.time()\n",
        "  cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=15), ShowGraph()]\n",
        "  #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "  learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "  kk=kk+1\n",
        "  learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "  elapsed = time.time() - start\n",
        "  vals = learn.recorder.values[-1]\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  results.loc[kk] = [kk, vals[0], vals[1], ac,pre,f1,auc,recal, int(elapsed)]\n",
        "  results.sort_values(by='kk', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "  clear_output()\n",
        "  display(results)"
      ],
      "metadata": {
        "id": "bkd1ULxXj-K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = get_splits(y, n_splits=5, shuffle=True, stratify=True)\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [ (FCN, {})]\n",
        "\n",
        "results = pd.DataFrame(columns=['kk','arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  #dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize()])\n",
        "  for i, (arch, k) in enumerate(archs):\n",
        "    model = create_model(arch, dls=dls, **k)\n",
        "    print(model.__class__.__name__)\n",
        "    learn = ts_learner(dls, model, metrics=accuracy)\n",
        "    start = time.time()\n",
        "    cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=20), ShowGraph()]\n",
        "    #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "    learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "    kk=kk+1\n",
        "    learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch.__name__, learner_fname='learner_'+str(kk)+arch.__name__)\n",
        "    elapsed = time.time() - start\n",
        "    vals = learn.recorder.values[-1]\n",
        "\n",
        "    dls2 = learn.dls\n",
        "    valid_dl = dls2.valid\n",
        "    #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "    #test_dl = valid_dl.new(valid_dl)\n",
        "    test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "    accurary=(test_targets == test_preds).float().mean()\n",
        "    ac=skm.accuracy_score(test_targets, test_preds)\n",
        "    pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "    f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "    auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "    recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "    results.loc[kk] = [kk,arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2],pre,f1,auc,recal, int(elapsed)]\n",
        "    results.sort_values(by='arch', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "    clear_output()\n",
        "    display(results)"
      ],
      "metadata": {
        "id": "qU_ioZseKYLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splits = get_splits(y, n_splits=5, shuffle=True, stratify=True)\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [ (ResCNN, {})]\n",
        "\n",
        "results = pd.DataFrame(columns=['kk','arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  #dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize()])\n",
        "  for i, (arch, k) in enumerate(archs):\n",
        "    model = create_model(arch, dls=dls, **k)\n",
        "    print(model.__class__.__name__)\n",
        "    learn = ts_learner(dls, model, metrics=accuracy)\n",
        "    start = time.time()\n",
        "    cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=30), ShowGraph()]\n",
        "    #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "    learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "    kk=kk+1\n",
        "    learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='32_16_3'+str(kk)+arch.__name__, learner_fname='learner_32_16_3_'+str(kk)+arch.__name__)\n",
        "    elapsed = time.time() - start\n",
        "    vals = learn.recorder.values[-1]\n",
        "\n",
        "    dls2 = learn.dls\n",
        "    valid_dl = dls2.valid\n",
        "    #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "    #test_dl = valid_dl.new(valid_dl)\n",
        "    interp = ClassificationInterpretation.from_learner(learn)\n",
        "    interp.plot_confusion_matrix()\n",
        "    test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "    accurary=(test_targets == test_preds).float().mean()\n",
        "    ac=skm.accuracy_score(test_targets, test_preds)\n",
        "    pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "    f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "    auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "    recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "    results.loc[kk] = [kk,arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2],pre,f1,auc,recal, int(elapsed)]\n",
        "    results.sort_values(by='arch', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "    #clear_output()\n",
        "    display(results)"
      ],
      "metadata": {
        "id": "N_2j1D9TsqU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title accuracy\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "results['accuracy'].plot(kind='line', figsize=(8, 4), title='accuracy')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "WhsmU8Q0-BXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AYe_VKb08d_"
      },
      "outputs": [],
      "source": [
        "bs = 64\n",
        "print(X.shape)\n",
        "tfms  = [None, [Categorize()]]\n",
        "\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
        "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "\n",
        "results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy', 'time'])\n",
        "for i, (arch, k) in enumerate(archs):\n",
        "    model = create_model(arch, dls=dls, **k)\n",
        "    print(model.__class__.__name__)\n",
        "    #metrics = [accuracy, precision_multi, recall_multi,  specificity_multi, F1_multi]\n",
        "    learn = ts_learner(dls, model,  metrics=accuracy)\n",
        "    start = time.time()\n",
        "    learn.fit_one_cycle(100, 1e-3)\n",
        "    learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_'+arch.__name__, learner_fname='learner_'+arch.__name__)\n",
        "    elapsed = time.time() - start\n",
        "    vals = learn.recorder.values[-1]\n",
        "    results.loc[i] = [arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2], int(elapsed)]\n",
        "    results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n",
        "    clear_output()\n",
        "    display(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#learn.save_all(path='export', dls_fname='dls', model_fname='model_'+arch.__name__, learner_fname='learner_'+arch.__name__)\n",
        "#del learn, dsets, dls\n",
        "rchs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss','accuracy','precision','f1','auc','recall'])\n",
        "for i, (arch, k) in enumerate(archs):\n",
        "  learn = load_learner_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_'+arch.__name__, learner_fname='learner_'+arch.__name__)\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  ac,pre,f1,auc,recal\n",
        "  model = create_model(arch, dls=dls, **k)\n",
        "  #interp = ClassificationInterpretation.from_learner(learn)\n",
        "  #interp.plot_confusion_matrix()\n",
        "  vals = learn.recorder.values[-1]\n",
        "  results.loc[i] = [arch.__name__, k, count_parameters(model), vals[0], vals[1], ac,pre,f1,auc,recal]\n",
        "  results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n",
        "  clear_output()\n",
        "  display(results)\n",
        "accurary,ac,pre,f1,auc,recal"
      ],
      "metadata": {
        "id": "8TSYKoeFJ4La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#*****************************************backup\n",
        "\n",
        "bs = 64\n",
        "print(X.shape)\n",
        "tfms  = [None, [Categorize()]]\n",
        "\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
        "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "#archs = [(LSTM_FCN, {})]\n",
        "\n",
        "results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "for i, (arch, k) in enumerate(archs):\n",
        "    model = create_model(arch, dls=dls, **k)\n",
        "    print(model.__class__.__name__)\n",
        "    metrics = [accuracy, precision_multi, recall_multi,  specificity_multi, F1_multi]\n",
        "    learn = ts_learner(dls, model,  metrics=accuracy)\n",
        "\n",
        "    start = time.time()\n",
        "    learn.fit_one_cycle(100, 1e-3)\n",
        "    learn.save_all(path='export', dls_fname='dls', model_fname='model_'+arch.__name__, learner_fname='learner_'+arch.__name__)\n",
        "    elapsed = time.time() - start\n",
        "    vals = learn.recorder.values[-1]\n",
        "    results.loc[i] = [arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2],pre,f1,auc,recal,int(elapsed)]\n",
        "    results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n",
        "    clear_output()\n",
        "    display(results)\n"
      ],
      "metadata": {
        "id": "hlVW2HjpnSja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64\n",
        "print(X.shape)\n",
        "tfms  = [None, [Categorize()]]\n",
        "\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
        "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "\n",
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(LSTM_Inception, {})]\n",
        "\n",
        "results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "for i, (arch, k) in enumerate(archs):\n",
        "    model = create_model(arch, dls=dls, **k)\n",
        "    print(model.__class__.__name__)\n",
        "    metrics = [accuracy, precision_multi, recall_multi,  specificity_multi, F1_multi]\n",
        "    learn = ts_learner(dls, model,  metrics=accuracy)\n",
        "    start = time.time()\n",
        "    learn.fit_one_cycle(100, 1e-3)\n",
        "    elapsed = time.time() - start\n",
        "    vals = learn.recorder.values[-1]\n",
        "    results.loc[i] = [arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2],pre,f1,auc,recal,int(elapsed)]\n",
        "    results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n",
        "    clear_output()\n",
        "    display(results)\n"
      ],
      "metadata": {
        "id": "PMU-YDZc0PIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified\n",
        "class MySuperCustomModel2(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        backbone = c_in\n",
        "        head = MySuperCustomModelHead(c_in, c_out, nf, nb_filters, adaptive_size, **kwargs)\n",
        "        super().__init__(OrderedDict([]))\n",
        "\n",
        "class MySuperCustomModel1(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.):\n",
        "        backbone = MySuperCustomLSTM(c_in, c_out, seq_len, hidden_size, rnn_layers, bias, cell_dropout, rnn_dropout, bidirectional, shuffle,fc_dropout)\n",
        "        #head = MySuperCustomModelHead(c_in, c_out, nf, nb_filters, adaptive_size, **kwargs)\n",
        "        super().__init__(OrderedDict([('backbone', backbone)]))\n",
        "\n",
        "class MySuperCustomModel3(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        backbone = MySuperCustomLSTM(c_in, c_out, seq_len, hidden_size, rnn_layers, bias, cell_dropout, rnn_dropout, bidirectional, shuffle,fc_dropout)\n",
        "        head = MySuperCustomModelHead(c_out, nf, nb_filters, adaptive_size, **kwargs)\n",
        "        super().__init__(OrderedDict([('backbone', backbone),('head', head)]))\n",
        "\n",
        "class MySuperCustomModel4(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        super(MySuperCustomModel4,self).__init__()\n",
        "        #self.lstm = nn.LSTM(input_size=c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.rnn = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        nf = ifnone(nf, nb_filters)\n",
        "        self.block = XceptionBlock(c_in, nf, **kwargs)\n",
        "        self.head_nf = nf * 32\n",
        "        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(adaptive_size),\n",
        "                                  ConvBlock(self.head_nf, self.head_nf//2, 1),\n",
        "                                  ConvBlock(self.head_nf//2, self.head_nf//4, 1),\n",
        "                                  ConvBlock(self.head_nf//4, c_out, 1),\n",
        "                                  GAP1d(1))\n",
        "    def forward(self, x):\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        output, _ = self.rnn(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "\n",
        "        x = self.block(output)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MySuperCustomModel2(nn.Sequential):\n",
        "\n",
        "    def __init__(self, c_in, c_out,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "\n",
        "        backbone = XceptionBlock(c_in, nf, **kwargs)\n",
        "        head = MySuperCustomModelHead(nf, c_out)\n",
        "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))\n",
        "\n",
        "\n",
        "class MySuperCustomModelHead(Module):\n",
        "\n",
        "    def __init__(self, nf, c_out, adaptive_size=50, **kwargs):\n",
        "        self.head_nf = nf * 32\n",
        "        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(adaptive_size),\n",
        "                                  ConvBlock(self.head_nf, self.head_nf//2, 1),\n",
        "                                  ConvBlock(self.head_nf//2, self.head_nf//4, 1),\n",
        "                                  ConvBlock(self.head_nf//4, c_out, 1),\n",
        "                                  GAP1d(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.head(x)\n",
        "        return x.clamp(max=1.0)\n",
        "\n",
        "class MySuperCustomLSTM(Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.):\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.rnn = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        output, _ = self.rnn(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "        #out, _ = self.lstm(x)\n",
        "        #out = out.permute(0, 2, 1)\n",
        "        print(output.shape)\n",
        "        return last_out"
      ],
      "metadata": {
        "id": "-dYTzu5U36vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified\n",
        "class MySuperCustomModel2(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        backbone = c_in\n",
        "        head = MySuperCustomModelHead(c_in, c_out, nf, nb_filters, adaptive_size, **kwargs)\n",
        "        super().__init__(OrderedDict([]))\n",
        "\n",
        "class MySuperCustomModel1(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.):\n",
        "        backbone = MySuperCustomLSTM(c_in, c_out, seq_len, hidden_size, rnn_layers, bias, cell_dropout, rnn_dropout, bidirectional, shuffle,fc_dropout)\n",
        "        #head = MySuperCustomModelHead(c_in, c_out, nf, nb_filters, adaptive_size, **kwargs)\n",
        "        super().__init__(OrderedDict([('backbone', backbone)]))\n",
        "\n",
        "class MySuperCustomModel3(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        backbone = MySuperCustomLSTM(c_in, c_out, seq_len, hidden_size, rnn_layers, bias, cell_dropout, rnn_dropout, bidirectional, shuffle,fc_dropout)\n",
        "        head = MySuperCustomModelHead(c_out, nf, nb_filters, adaptive_size, **kwargs)\n",
        "        super().__init__(OrderedDict([('backbone', backbone),('head', head)]))\n",
        "\n",
        "class MySuperCustomModel4(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=64, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "        super(MySuperCustomModel4,self).__init__()\n",
        "        #self.lstm = nn.LSTM(input_size=c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.rnn = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        nf = ifnone(nf, nb_filters)\n",
        "        self.block = XceptionBlock(hidden_size, nf, **kwargs)\n",
        "        self.head_nf = nf * 32\n",
        "        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(adaptive_size),\n",
        "                                  ConvBlock(self.head_nf, self.head_nf//2, 1),\n",
        "                                  ConvBlock(self.head_nf//2, self.head_nf//4, 1),\n",
        "                                  ConvBlock(self.head_nf//4, c_out, 1),\n",
        "                                  GAP1d(1))\n",
        "    def forward(self, x):\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        output, _ = self.rnn(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "        output=self.shuffle(output)\n",
        "        x = self.block(output)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MySuperCustomModel2(nn.Sequential):\n",
        "\n",
        "    def __init__(self, c_in, c_out,  nf=16, nb_filters=None, adaptive_size=50, **kwargs):\n",
        "\n",
        "        backbone = XceptionBlock(c_in, nf, **kwargs)\n",
        "        head = MySuperCustomModelHead(nf, c_out)\n",
        "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))\n",
        "\n",
        "\n",
        "class MySuperCustomModelHead(Module):\n",
        "\n",
        "    def __init__(self, nf, c_out, adaptive_size=50, **kwargs):\n",
        "        self.head_nf = nf * 32\n",
        "        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(adaptive_size),\n",
        "                                  ConvBlock(self.head_nf, self.head_nf//2, 1),\n",
        "                                  ConvBlock(self.head_nf//2, self.head_nf//4, 1),\n",
        "                                  ConvBlock(self.head_nf//4, c_out, 1),\n",
        "                                  GAP1d(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.head(x)\n",
        "        return x.clamp(max=1.0)\n",
        "\n",
        "class MySuperCustomLSTM(Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False, shuffle=False,\n",
        "                 fc_dropout=0.):\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.rnn = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        output, _ = self.rnn(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "        #out, _ = self.lstm(x)\n",
        "        #out = out.permute(0, 2, 1)\n",
        "        print(output.shape)\n",
        "        return last_out"
      ],
      "metadata": {
        "id": "pb81CimWKjP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Inception(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1, bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=True, shuffle=False,\n",
        "                 fc_dropout=0.,  nf=32, nb_filters=None,  **kwargs):\n",
        "        super(LSTM_Inception,self).__init__()\n",
        "        #self.lstm = nn.LSTM(input_size=c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        if shuffle: assert seq_len is not None, 'need seq_len if shuffle=True'\n",
        "        self.LSTM = nn.LSTM(seq_len if shuffle else c_in, hidden_size, num_layers=rnn_layers, bias=bias, batch_first=True,\n",
        "                              dropout=cell_dropout, bidirectional=bidirectional)\n",
        "        self.rnn_dropout = nn.Dropout(rnn_dropout) if rnn_dropout else noop\n",
        "        self.shuffle = Permute(0,2,1) if not shuffle else noop\n",
        "        #self.lstm = nn.LSTM(c_in, hidden_size=hidden_size, num_layers=rnn_layers, batch_first=True)\n",
        "\n",
        "        #Inception\n",
        "        nf = ifnone(nf, nb_filters) # for compatibility\n",
        "        self.inceptionblock = InceptionBlock(c_in, nf, **kwargs)\n",
        "        self.gap = GAP1d(1)\n",
        "        self.fc = nn.Linear(nf * 4, c_out)\n",
        "\n",
        "        # Common\n",
        "        self.concat = Concat()\n",
        "        self.fc_dropout = nn.Dropout(fc_dropout) if fc_dropout else noop\n",
        "        self.fc2 = nn.Linear(hidden_size * (1 + bidirectional) + c_out, c_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # xCEPTION\n",
        "        x1 = self.inceptionblock(x)\n",
        "        x2 = self.gap(x1)\n",
        "        y = self.fc(x2)\n",
        "        #print(x.shape)\n",
        "\n",
        "        # LSTM\n",
        "        rnn_input = self.shuffle(x) # permute --> (batch_size, seq_len, n_vars) when batch_first=True\n",
        "        #print(rnn_input.shape)\n",
        "        output, _ = self.LSTM(rnn_input)\n",
        "        last_out = output[:, -1] # output of last sequence step (many-to-one)\n",
        "        last_out = self.rnn_dropout(last_out)\n",
        "        #print(last_out.shape)\n",
        "\n",
        "        # Concat\n",
        "        x_con = self.concat([last_out, y])\n",
        "        x_con = self.fc_dropout(x_con)\n",
        "        x_con = self.fc2(x_con)\n",
        "\n",
        "        return x_con"
      ],
      "metadata": {
        "id": "WfiFSI3u4G_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = get_splits(y, n_splits=5, shuffle=True, stratify=True)\n",
        "#**************************************************************************************************************\n"
      ],
      "metadata": {
        "id": "qWyQ4pMRG_-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(XceptionTime, {})]\n",
        "arch_name='LSTM_Inception_dep3_ks8_nf32'\n",
        "depth=3\n",
        "ks=8\n",
        "nf=32\n",
        "results = pd.DataFrame(columns=['kk', 'train loss', 'valid loss', 'actrain','accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  c_in = 33  # Number of input features\n",
        "  c_out = 5  # Number of output features\n",
        "  model = LSTM_Inception(c_in=c_in, c_out=c_out, nf=nf,depth=depth,ks=ks)\n",
        "  #model = LSTM_Inception(nf=17)\n",
        "  learn = ts_learner(dls, model, metrics=accuracy)\n",
        "  learn.recorder.train_metrics = True\n",
        "  learn.recorder.valid_metrics = True\n",
        "  start = time.time()\n",
        "  cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=30), ShowGraph()]\n",
        "  #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "  learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "  kk=kk+1\n",
        "  #learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "  elapsed = time.time() - start\n",
        "  vals = learn.recorder.values[-1]\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  train_dl = dls2.train\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  interp = ClassificationInterpretation.from_learner(learn)\n",
        "  interp.plot_confusion_matrix()\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  train_probas, train_targets, train_preds=learn.get_preds(dl=train_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  actrain=skm.accuracy_score(train_targets, train_preds)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  results.loc[kk] = [kk, vals[0], vals[1],actrain, ac,pre,f1,auc,recal, int(elapsed)]\n",
        "  results.sort_values(by='kk', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "  #clear_output()\n",
        "  display(results)\n",
        "  accurary,ac,pre,f1,auc,recal"
      ],
      "metadata": {
        "id": "ZDLfcoVFbBJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arch_name='LSTTM_Inception_contact'\n",
        "kk=3\n",
        "learn = load_learner_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix()\n"
      ],
      "metadata": {
        "id": "gdEQELN1eNGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(XceptionTime, {})]\n",
        "arch_name='LSTM_Inception_dep3_ks16_nf8'\n",
        "depth=3\n",
        "ks=16\n",
        "nf=8\n",
        "results = pd.DataFrame(columns=['kk', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  c_in = 33  # Number of input features\n",
        "  c_out = 5  # Number of output features\n",
        "  model = LSTM_Inception(c_in=c_in, c_out=c_out, nf=nf,depth=depth,ks=ks)\n",
        "  #model = LSTM_Inception(nf=17)\n",
        "  learn = ts_learner(dls, model, metrics=accuracy)\n",
        "  start = time.time()\n",
        "  cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=30), ShowGraph()]\n",
        "  #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "  learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "  kk=kk+1\n",
        "  learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "  elapsed = time.time() - start\n",
        "  vals = learn.recorder.values[-1]\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  interp = ClassificationInterpretation.from_learner(learn)\n",
        "  interp.plot_confusion_matrix()\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  results.loc[kk] = [kk, vals[0], vals[1], ac,pre,f1,auc,recal, int(elapsed)]\n",
        "  results.sort_values(by='kk', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "  #clear_output()\n",
        "  display(results)\n",
        "  accurary,ac,pre,f1,auc,recal"
      ],
      "metadata": {
        "id": "9gXGFMW4hxvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(XceptionTime, {})]\n",
        "arch_name='LSTM_Inception_dep3_ks16_nf16'\n",
        "depth=3\n",
        "ks=16\n",
        "nf=16\n",
        "results = pd.DataFrame(columns=['kk', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  c_in = 33  # Number of input features\n",
        "  c_out = 5  # Number of output features\n",
        "  model = LSTM_Inception(c_in=c_in, c_out=c_out, nf=nf,depth=depth,ks=ks)\n",
        "  #model = LSTM_Inception(nf=17)\n",
        "  learn = ts_learner(dls, model, metrics=accuracy)\n",
        "  start = time.time()\n",
        "  cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=70), ShowGraph()]\n",
        "  #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "  learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "  kk=kk+1\n",
        "  learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "  elapsed = time.time() - start\n",
        "  vals = learn.recorder.values[-1]\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  interp = ClassificationInterpretation.from_learner(learn)\n",
        "  interp.plot_confusion_matrix()\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  results.loc[kk] = [kk, vals[0], vals[1], ac,pre,f1,auc,recal, int(elapsed)]\n",
        "  results.sort_values(by='kk', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "  #clear_output()\n",
        "  display(results)\n",
        "  accurary,ac,pre,f1,auc,recal"
      ],
      "metadata": {
        "id": "2OMathRqh1Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archs = [(FCN, {}), (ResNet, {}), (ResCNN, {}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}),\n",
        "         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}), (LSTM, {'n_layers':3, 'bidirectional': True})]\n",
        "\n",
        "archs = [(LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False}), (InceptionTime, {}), (XceptionTime, {}), (OmniScaleCNN, {}), (mWDN, {'levels': 4})]\n",
        "archs = [(XceptionTime, {})]\n",
        "arch_name='LSTM_Inception_dep3_ks16_nf64'\n",
        "depth=3\n",
        "ks=16\n",
        "nf=64\n",
        "results = pd.DataFrame(columns=['kk', 'train loss', 'valid loss', 'accuracy','precision','f1','auc','recall','time'])\n",
        "kk=0\n",
        "for s in splits:\n",
        "  bs = 32\n",
        "  print(X.shape)\n",
        "  tfms  = [None, [Categorize()]]\n",
        "  dsets = TSDatasets(X, y, tfms=tfms, splits=s)\n",
        "  dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2],batch_tfms=[TSNormalize(range=(-1, 1))])\n",
        "  c_in = 33  # Number of input features\n",
        "  c_out = 5  # Number of output features\n",
        "  model = LSTM_Inception(c_in=c_in, c_out=c_out, nf=nf,depth=depth,ks=ks)\n",
        "  #model = LSTM_Inception(nf=17)\n",
        "  learn = ts_learner(dls, model, metrics=accuracy)\n",
        "  start = time.time()\n",
        "  cbs = [EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001,patience=60), ShowGraph()]\n",
        "  #monitor='error_rate', min_delta=1e-3, patience=5\n",
        "  learn.fit_one_cycle(100, lr_max=1e-3, cbs=cbs)\n",
        "  kk=kk+1\n",
        "  learn.save_all(path='/content/drive/MyDrive/TSAI_NEWexport', dls_fname='dls', model_fname='model_Fal'+str(kk)+arch_name, learner_fname='learner_'+str(kk)+arch_name)\n",
        "  elapsed = time.time() - start\n",
        "  vals = learn.recorder.values[-1]\n",
        "  dls2 = learn.dls\n",
        "  valid_dl = dls2.valid\n",
        "  #test_ds = valid_dl.dataset.add_test(X[splits[1]], y[splits[1]])# In this case I'll use X and y, but this would be your test data\n",
        "  #test_dl = valid_dl.new(valid_dl)\n",
        "  interp = ClassificationInterpretation.from_learner(learn)\n",
        "  interp.plot_confusion_matrix()\n",
        "  test_probas, test_targets, test_preds = learn.get_preds(dl=valid_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
        "  accurary=(test_targets == test_preds).float().mean()\n",
        "  ac=skm.accuracy_score(test_targets, test_preds)\n",
        "  pre=skm.precision_score(test_targets, test_preds, average='weighted')\n",
        "  f1=skm.f1_score(test_targets, test_preds, average='weighted')\n",
        "  auc=skm.roc_auc_score(test_targets,test_probas,average='weighted',multi_class='ovo')\n",
        "  recal=skm.recall_score(test_targets, test_preds, average='weighted')\n",
        "  results.loc[kk] = [kk, vals[0], vals[1], ac,pre,f1,auc,recal, int(elapsed)]\n",
        "  results.sort_values(by='kk', ascending=True, kind='stable', ignore_index=True, inplace=True)\n",
        "  #clear_output()\n",
        "  display(results)\n",
        "  accurary,ac,pre,f1,auc,recal"
      ],
      "metadata": {
        "id": "gjaJsX1Gh3ts"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "17tZ51jIMCV16P7syzgBzk5hP2CVVnCtO",
      "authorship_tag": "ABX9TyPKiTu6/PfzhjGZ5CIQVyJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}